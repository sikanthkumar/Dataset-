import numpy as np

import matplotlib.pyplot as plt

from sklearn import datasets

from sklearn import svm

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score

# Load the Iris dataset

iris = datasets.load_iris()

X = iris.data

y = iris.target

# Split the dataset into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create an SVM classifier

clf = svm.SVC(kernel='linear', C=1)

clf.fit(X_train, y_train)

# Predict on the test set

y_pred = clf.predict(X_test)

# Calculate accuracy

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")

# Visualization (optional)

plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap=plt.cm.coolwarm)

plt.xlabel('Sepal Length')

plt.ylabel('Sepal Width')

plt.title('SVM Classification of Iris')

plt.show()





import numpy as np

import matplotlib.pyplot as plt

from sklearn.datasets import make_circles

from sklearn.model_selection import train_test_split

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense

from tensorflow.keras.optimizers import Adam

# Create the dataset

X, y = make_circles(n_samples=1000, noise=0.03, random_state=42)

# Split the dataset into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)

# Build and train the model

model = Sequential([

Dense(16, activation='relu', input_shape=(2,)),

Dense(8, activation='relu'),

Dense(1, activation='sigmoid')

])

model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10)

# Evaluate the model

loss, accuracy = model.evaluate(X_test, y_test)

print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')

# Create a plot to visualize the decision boundary

def plot_decision_boundary(X, y, model):

xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 100),

                      np.linspace(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 100))

xy = np.c_[xx.ravel(), yy.ravel()]

Z = model.predict(xy).reshape(xx.shape)

plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.7)

plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)

# Plot the decision boundaries for both training and testing data

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)

plt.title('Training')

plot_decision_boundary(X_train, y_train, model)

plt.subplot(1, 2, 2)

plt.title('Testing')

plot_decision_boundary(X_test, y_test, model)

plt.show()





import numpy as np

from sklearn.mixture import GaussianMixture

import matplotlib.pyplot as plt

# Generate some sample data

np.random.seed(0)

X = np.concatenate([np.random.randn(100, 2), 10 + np.random.randn(100, 2)])

# Create and fit the GMM model

gmm = GaussianMixture(n_components=2)

gmm.fit(X)

# Predict the cluster assignments for each data point

labels = gmm.predict(X)

# Plot the data points and their cluster assignments

plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')

plt.xlabel('X-axis')

plt.ylabel('Y-axis')

plt.title('Gaussian Mixture Model Clustering')

plt.show()

from sklearn.cluster import KMeans

import matplotlib.pyplot as plt

X = ...  # Your data

k = 3  # Number of clusters

labels = KMeans(n_clusters=k).fit_predict(X)

centers = KMeans(n_clusters=k).fit(X).cluster_centers_

plt.scatter(X[:, 0], X[:, 1], c=labels)

plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200, label='Cluster Centers')

plt.show()





from sklearn.decomposition import PCA

from sklearn.datasets import load_iris

import matplotlib.pyplot as plt

# Load the Iris dataset

data = load_iris()

X = data.data

# Perform PCA with 2 components

pca = PCA(n_components=2)

X_r = pca.fit_transform(X)

# Plot the PCA results

plt.scatter(X_r[:, 0], X_r[:, 1], c=data.target)

plt.xlabel('Principal Component 1')

plt.ylabel('Principal Component 2')

plt.title('PCA of IRIS dataset')

plt.show()





import numpy as np

import matplotlib.pyplot as plt

from scipy.stats import multivariate_normal

# Define the mean and covariance matrix for a 2D multivariate Gaussian

mean = np.array([0, 0])

cov_matrix = np.array([[1, 0.5], [0.5, 1]])

# Generate a grid of x and y values

x, y = np.mgrid[-3:3:0.01, -3:3:0.01]

pos = np.dstack((x, y))

# Calculate the probability density for each (x, y) pair

pdf = multivariate_normal(mean, cov_matrix).pdf(pos)

# Create a contour plot

plt.contourf(x, y, pdf, cmap='viridis')

plt.xlabel('X')

plt.ylabel('Y')

plt.title('Bivariate Gaussian Density Function')

plt.colorbar()

plt.show()





import numpy as np

import matplotlib.pyplot as plt

from scipy.stats import norm

mean, variance = 0, 1

x = np.linspace(-5, 5, 100)

pdf = norm.pdf(x, loc=mean, scale=np.sqrt(variance))

plt.plot(x, pdf)

plt.xlabel('x')

plt.ylabel('Probability Density')

plt.title('Univariate Gaussian Density Function')

plt.show()





# Import necessary libraries

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

from sklearn.metrics import mean_squared_error

# Load the example dataset

data = pd.read_csv('example_dataset.csv')

# Assuming your dataset has features (X) and target variable (Y)

X = data['X'].values.reshape(-1, 1)

y = data['Y'].values

# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create a Linear Regression model

linear_model = LinearRegression()

# Fit the model to the training data

linear_model.fit(X_train, y_train)

# Make predictions on the testing data

linear_predictions = linear_model.predict(X_test)

# Calculate error estimates (mean squared error)

linear_mse = mean_squared_error(y_test, linear_predictions)

# Visualize the data and the regression line

plt.scatter(X, y, color='blue', label='Data Points')

plt.plot(X_test, linear_predictions, color='red', label='Regression Line')

plt.title("Linear Regression")

plt.xlabel("X")

plt.ylabel("Y")

plt.legend()

plt.show()

# Print error estimate (mean squared error)

print("Linear Regression - Mean Squared Error:", linear_mse)

X1,X2,Target

1,2,0

2,3,0

3,4,1

4,5,1

5,6,1





# Import necessary libraries

import pandas as pd

import numpy as np

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

import matplotlib.pyplot as plt

# Load your dataset (replace 'your_dataset.csv' with your dataset file)

data = pd.read_csv('your_dataset.csv')

# Assuming your dataset has features (X) and target variable (y)

X = data.drop('Target', axis=1)  # Replace 'Target' with your target column name

y = data['Target']

# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Create a Logistic Regression model

logistic_model = LogisticRegression()

# Fit the model to the training data

logistic_model.fit(X_train, y_train)

# Make predictions on the testing data

logistic_predictions = logistic_model.predict(X_test)

# Calculate error estimates (accuracy, confusion matrix, and classification report)

logistic_accuracy = accuracy_score(y_test, logistic_predictions)

confusion = confusion_matrix(y_test, logistic_predictions)

classification_rep = classification_report(y_test, logistic_predictions)

# Print error estimates

print("Logistic Regression - Accuracy:", logistic_accuracy)

# Print confusion matrix

print("Confusion Matrix:")

print(confusion)

# Print classification report

print("Classification Report:")

print(classification_rep)

# Visualization

plt.scatter(X_test['X1'], X_test['X2'], c=y_test, cmap='viridis')

plt.xlabel('X1')

plt.ylabel('X2')

plt.title('Logistic Regression - Decision Boundary')

plt.show()





from sklearn.tree import DecisionTreeClassifier

from sklearn.datasets import load_iris

from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report, confusion_matrix

import matplotlib.pyplot as plt

import seaborn as sns

# Load the Iris dataset as an example

iris = load_iris()

X = iris.data

y = iris.target

# Split the data into a training and testing set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a decision tree classifier

clf = DecisionTreeClassifier()

clf.fit(X_train, y_train)

# Make predictions on the test set

y_pred = clf.predict(X_test)

# Generate a classification report

report = classification_report(y_test, y_pred, target_names=iris.target_names)

print("Classification Report:\n", report)

# Generate a confusion matrix

cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using a heatmap

plt.figure(figsize=(8, 6))

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)

plt.xlabel('Predicted')

plt.ylabel('Actual')

plt.title('Confusion Matrix')

plt.show()





from sklearn.ensemble import RandomForestClassifier

from sklearn.datasets import load_iris

from sklearn.model_selection import train_test_split

from sklearn.metrics import classification_report, confusion_matrix

import matplotlib.pyplot as plt

import seaborn as sns

# Load the Iris dataset as an example

iris = load_iris()

X = iris.data

y = iris.target

# Split the data into a training and testing set

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest classifier

clf = RandomForestClassifier(n_estimators=100, random_state=0)

clf.fit(X_train, y_train)

# Make predictions on the test set

y_pred = clf.predict(X_test)

# Generate a classification report

report = classification_report(y_test, y_pred, target_names=iris.target_names)

print("Classification Report:\n", report)

# Generate a confusion matrix

cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using a heatmap

plt.figure(figsize=(8, 6))

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)

plt.xlabel('Predicted')

plt.ylabel('Actual')

plt.title('Confusion Matrix')

plt.show()





import numpy as np

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

from tensorflow.keras.datasets import mnist

import matplotlib.pyplot as plt

# Load and preprocess the MNIST dataset

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train, X_test = X_train / 255.0, X_test / 255.0

# Create a simple CNN model

model = Sequential([

Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),

MaxPooling2D((2, 2)),

Flatten(),

Dense(128, activation='relu'),

Dense(10, activation='softmax')

])

# Compile the model

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model and record training history

history = model.fit(X_train.reshape(-1, 28, 28, 1), y_train, epochs=5, validation_data=(X_test.reshape(-1, 28, 28, 1), y_test))

# Visualize training history (accuracy and loss)

plt.plot(history.history['accuracy'], label='Train Accuracy')

plt.plot(history.history['val_accuracy'], label='Test Accuracy')

plt.xlabel('Epoch')

plt.ylabel('Accuracy')

plt.legend()

plt.show()





import numpy as np

import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import SimpleRNN, Dense

# Generate a sample sequence data

sequence_length = 10

X = np.random.randn(100, sequence_length, 1)

y = np.sum(X, axis=1)

# Create a simple RNN model

model = Sequential()

model.add(SimpleRNN(units=64, input_shape=(sequence_length, 1))

model.add(Dense(1))

# Compile the model

model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model

history = model.fit(X, y, epochs=10)

# Predict the values

y_pred = model.predict(X)

# Visualize the true and predicted values

plt.plot(y, label='True Values')

plt.plot(y_pred, label='Predicted Values', linestyle='--')

plt.xlabel('Sample Index')

plt.ylabel('Value')

plt.legend()

plt.title('Sequence Prediction with RNN')

plt.show()


